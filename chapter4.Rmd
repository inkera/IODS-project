---
title: "Chapter 4"
author: "Laura Jernst√∂m"
date: "11/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```



```{r, message = FALSE}


rm(list=ls())

# 1.

library(MASS)
library(tidyr)

data(Boston)

str(Boston)
dim(Boston)



```
The Boston data set contains information on housing Values in suburbs of Boston and includes 506 observations of 14 variables. There are information including crime rate, proportion on residential land zoned for lots, nitrogen oxiden concentration, age and size of living units, residential taxes and pupil-teacher ratio.

```{r, message = FALSE}


#install.packages("corrplot")
library(corrplot)
library(GGally)

# Summaries of the variables
summary(Boston)

p1 <- pairs(Boston, main = "Figure 1")

cor_matrix<-cor(Boston) 

# create a more advanced plot matrix with ggpairs()
p2a <- ggpairs(Boston[1:7], lower = list(combo = wrap("facethist", bins = 20)), title = "Figure 2a")

# create a more advanced plot matrix with ggpairs()
p2b <- ggpairs(Boston[8:14], lower = list(combo = wrap("facethist", bins = 20)), title = "Figure 2b")


# draw the plots
p1
p2a
p2b

# print the correlation matrix
cor_matrix %>% round(digits=2)

# visualize the correlation matrix
p3 <- corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6, main = "Figure 3")
p3


```

Looking at Figure 1 and 2, we can see that the distributions of the variables are quite skewed in most cases with the expection of the variable rm, which has a distribution closer to the normal distribution. The variables indust, tax, rad ( and ptratio?) also have multiple peeks in their distriputions.

From Figure 3 we can see correlations between variables. Variable pairs rad and tax, rm and medv, indus and tax, indus and nox, nox and age are strongly positive correlated with correlation coefficients close to 1. Variable pairs indus and dis, indus and nox, indus and age, lstat and medv are strongly negatively correlated with coefficients close to -1. Other variable pairs less strongly correlated with the variable chas being the most linearly independent variable.


```{r, message = FALSE}


# 4.

# Scale by substracting the mean and dividing with the standard deviation.

boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)


# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)




```

The change to previous can be seen from the summary statistics: The means are now standardized to zero.


```{r, results = 'hide', message = FALSE}


# 4. continues

# Categorical variable crime rate and division into training and testing sets.

summary(boston_scaled$crim)

# create a quantile vector of crim and print it


bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


## Division into training and testing

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)




```



```{r, results = 'hide', message = FALSE}

# 5. 

# MASS and train are available

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results  (LDA (bi)plot)
plot(lda.fit, dimen = 2, col = classes, pch = classes, main = "Figure 4")
lda.arrows(lda.fit, myscale = 1)


```




```{r, message = FALSE}

# 6. 

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)



```


Looking at the cross tabulation of correct and predicted classes, the test set seams to perform relatively well, with mostly quite small deviations from the correct values, with the pair med_low/med_high somewhat of an exception.


```{r, results = 'hide', message = FALSE}

# 7. Distance measures (K-mass, euclidian, manhattan)


library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

# Scale the variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# k-means clustering
km <- kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
p51 <- pairs(boston_scaled[1:5], col = km$cluster, main = "Figure 5a")
p52 <- pairs(boston_scaled[6:10], col = km$cluster, main = "Figure 5b")


p51
p52


```



```{r, results = 'hide', message = FALSE}

# 7. Continues

# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
p6 <- qplot(x = 1:k_max, y = twcss, geom = 'line', main = "Figure 6")
p6


# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
p71 <- pairs(boston_scaled[1:5], col = km$cluster, main = "Figure 7a")
p72 <- pairs(boston_scaled[6:10], col = km$cluster, main = "Figure 7b")

p71
p72



```

Looking at Figure 5, it would appear that for most variables, the optimal number of clusters could be at k=2 or k=3. 

Looking at Figure 6, 7a and 7b, there can be seen elbows (kinks) at k= 2, k=3 and k=5, which could be optimal numbers of clusters.


***

